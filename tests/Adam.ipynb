{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e4bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import healpy as hp\n",
    "import torch\n",
    "import math\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy        import units, constants\n",
    "import astropy.constants as constants\n",
    "from p3droslo.plot  import plot_cube_2D, plot_spectrum\n",
    "from p3droslo.model import TensorModel\n",
    "from p3droslo.lines import Line\n",
    "from p3droslo.haar  import Haar\n",
    "import copy\n",
    "#check whether the version is correct\n",
    "#the latest version should be 0.0.15, at time of writing\n",
    "import p3droslo\n",
    "print(p3droslo.__version__)\n",
    "from astroquery.lamda import Lamda\n",
    "CC  = constants.c  .si.value   # Speed of light       [m/s]\n",
    "HH  = constants.h  .si.value   # Planck's constant    [J s]\n",
    "KB  = constants.k_B.si.value   # Boltzmann's constant [J/K]\n",
    "AMU = constants.u  .si.value   # Atomic mass unit     [kg]\n",
    "lambda_file='test.txt'\n",
    "line = Line(\n",
    "        species_name = \"test\",\n",
    "        transition   = 0,\n",
    "        datafile     = lambda_file,\n",
    "        molar_mass   = 1.0\n",
    ")\n",
    "def nH2 (r,nH2_in):\n",
    "    return nH2_in * np.power(r_in/r, 2.0)\n",
    "\n",
    "get_X_mol = {\n",
    "    'a' : 1.0e-8,\n",
    "    'b' : 1.0e-6\n",
    "}\n",
    "npoints   = 20\n",
    "length = npoints#number of voxels along each axis\n",
    "temp   =  20.00   # [K]\n",
    "vturb = 150 #[m/s]\n",
    "#dv = 100 #[m/s]\n",
    "r_in   = 1.0E13   # [m]\n",
    "r_out  = 7.8E16   # [m]\n",
    "\n",
    "nH2_in = 2.0E13   # [m^-3]\n",
    "nfreq = 21   # number of frequencies\n",
    "last_axis=2\n",
    "\n",
    "#R=np.linspace(-r_out, r_out, npoints)\n",
    "r_s= 2*1.0E14# 7.8E16   # [m]\n",
    "R=np.linspace(-r_s, r_s, npoints)\n",
    "R=torch.tensor(R,dtype=torch.float32)\n",
    "xx, yy, zz = torch.meshgrid(R, R, R,indexing='xy')\n",
    "#xx, yy, zz = torch.meshgrid(R, R, R)\n",
    "r = torch.sqrt(xx**2 + yy**2 + zz**2)\n",
    "#n_mol=get_X_mol['a']*torch.where((r>=r_in)&(r<=r_out),\n",
    "#                                nH2(r,nH2_in),\n",
    "#                                 torch.tensor(0.0))\n",
    "\n",
    "\n",
    "n_mol=get_X_mol['a']*torch.where((r>=r_in)&(r<=r_s),\n",
    "                                nH2(r,nH2_in),\n",
    "                                 torch.tensor(0.0))\n",
    "n_mol=get_X_mol['a']*nH2(r,nH2_in)\n",
    "\n",
    "model = TensorModel(shape=(length,length,length), sizes=3*(2*r_out,), dtau_warning_threshold=0.1)\n",
    "dz=abs(R[1]-R[0])\n",
    "##dz=abs(r[:,:,1:]-r[:,:,:-1])\n",
    "#dz=model.dx(2)\n",
    "#######################################\n",
    "#n_mol=get_X_mol['a']*nH2(r,nH2_in)\n",
    "n_mol=get_X_mol['a']*torch.where((r>=r_in)&(r<=r_s),\n",
    "                                nH2(r,nH2_in),\n",
    "                                 torch.tensor(0.0))\n",
    "model['density']=n_mol#*torch.ones(model.shape)\n",
    "model['temperature']=torch.full_like(r,temp)\n",
    "model['v_turbulence'] = torch.full_like(r,vturb)\n",
    "model['velocity_los'] = torch.full_like(r,0) #in m/s\n",
    "\n",
    "model['temperature']=torch.where((r>=r_in)&(r<=r_s),\n",
    "                                 temp,\n",
    "                                 torch.tensor(0.0))\n",
    "model['temperature']=torch.full_like(r,temp)\n",
    "model['v_turbulence']=torch.where((r>=r_in)&(r<=r_s),\n",
    "                                 vturb,\n",
    "                                 torch.tensor(0.0))\n",
    "model['v_turbulence'] = torch.full_like(r,vturb)\n",
    "model['velocity_x:v_max'] = 0*model['velocity_los']\n",
    "model['velocity_y:v_max'] = 0*model['velocity_los']\n",
    "model['velocity_z:v_max'] = model['velocity_los']\n",
    "delta_nu=2e6\n",
    "v_los_min=torch.min(model['velocity_los'])\n",
    "v_los_max=torch.max(model['velocity_los'])\n",
    "line_doppler=line.frequency*(1+model['velocity_los']/constants.c.value)\n",
    "fmin_doppler=line.frequency*(1+(v_los_min)/constants.c.value)-delta_nu\n",
    "fmax_doppler=line.frequency*(1+(v_los_max)/constants.c.value)+delta_nu\n",
    "frequencies=np.linspace(fmin_doppler, fmax_doppler, nfreq)\n",
    "frequencies=torch.tensor(frequencies,dtype=torch.float32)\n",
    "K_21=2e-16\n",
    "C_ij=nH2(r,nH2_in)*K_21\n",
    "C_ji=C_ij*(line.weight[1] / line.weight[0])*torch.exp(-(line.energy[1]-line.energy[0])/(KB*model['temperature'])) \n",
    "print(frequencies)\n",
    "\n",
    "import NLTE\n",
    "from RotatingModel import RotationModel\n",
    "def rotate_scalar(scalar, center, R):\n",
    "    #print(\"旋转前: \", torch.isnan(scalar).any())\n",
    "    dim_x, dim_y, dim_z = scalar.shape\n",
    "    rotated_scalar = torch.zeros_like(scalar)\n",
    "    R_inverse = R.T\n",
    "    x, y, z = torch.meshgrid(torch.arange(dim_x), torch.arange(dim_y), torch.arange(dim_z), indexing='xy')\n",
    "    x_centered = x - center[0]\n",
    "    y_centered = y - center[1]\n",
    "    z_centered = z - center[2]\n",
    "    x_old, y_old, z_old = torch.matmul(R_inverse, torch.stack([x_centered.flatten(), \n",
    "                                                              y_centered.flatten(), \n",
    "                                                              z_centered.flatten()]))\n",
    "    x_old += center[0]\n",
    "    y_old += center[1]\n",
    "    z_old += center[2]\n",
    "    x_old = torch.round(x_old).to(dtype=torch.int64)\n",
    "    y_old = torch.round(y_old).to(dtype=torch.int64)\n",
    "    z_old = torch.round(z_old).to(dtype=torch.int64)\n",
    "\n",
    "    mask = (x_old >= 0) & (x_old < dim_x) & (y_old >= 0) & (y_old < dim_y) & (z_old >= 0) & (z_old < dim_z)\n",
    "    rotated_scalar[x.ravel()[mask], \n",
    "                   y.ravel()[mask], \n",
    "                   z.ravel()[mask]] = scalar[x_old[mask], y_old[mask], z_old[mask]]\n",
    "    #print(\"旋转后: \", torch.isnan(rotated_scalar).any())\n",
    "    mask_region = torch.zeros_like(scalar, dtype=bool)\n",
    "    mask_region[x.ravel()[mask], y.ravel()[mask], z.ravel()[mask]] = True\n",
    "    if torch.isnan(rotated_scalar).any():\n",
    "        print(\"...NaN values detected in rotate_scalar\")\n",
    "    return rotated_scalar\n",
    "def compute_mean_intensity(model,pops,line,nfreq,frequencies):\n",
    "    #n_theta=100\n",
    "    #n_phi=100\n",
    "    NSIDE = 3\n",
    "    nrays = hp.nside2npix(NSIDE)\n",
    "    #print(nrays)\n",
    "    delta_omega= hp.nside2pixarea(NSIDE)\n",
    "    #delta_omega = hp.pixelfunc.nside2pixarea(NSIDE)\n",
    "    theta_, phi_ = hp.pix2ang(NSIDE, np.arange(nrays))\n",
    "    threshold = 1e-10\n",
    "    tau=torch.zeros([model.shape[0],model.shape[1],model.shape[2],frequencies.shape[0]])\n",
    "    dtau=torch.zeros([model.shape[0],model.shape[1],model.shape[2]-1,frequencies.shape[0]])\n",
    "    source=torch.zeros(model.shape)\n",
    "    img=torch.zeros([model.shape[0],model.shape[1],model.shape[2],frequencies.shape[0]])\n",
    "    J=torch.zeros(model.shape)\n",
    "    mean_intensity=torch.zeros(model.shape)\n",
    "    #print(f'nrays:{nrays}')\n",
    "    for theta,phi in zip(theta_,phi_):\n",
    "        #print(f'theta:{theta/(np.pi)},phi:{phi/np.pi}')\n",
    "        RM = RotationModel(model,pops,theta,phi,padding=False)\n",
    "        #RM = RotationModel(model,pops,0, 0,padding=False)\n",
    "        #print('*********check1****************************')\n",
    "        mask=RM.mask_region\n",
    "        sigma=torch.zeros(model.shape)\n",
    "        #sigma = sigma.clone()\n",
    "        sigma[mask]=NLTE.compute_sigma(line,                                       \n",
    "                                  RM['temperature'][mask],\n",
    "                                  RM['v_turbulence'][mask]).float()\n",
    "        line_doppler=line.frequency*(1+RM['velocity_los']/constants.c.value)\n",
    "        line_doppler=line_doppler.float()\n",
    "        profile=torch.zeros([model.shape[0],model.shape[1],model.shape[2],frequencies.shape[0]])\n",
    "        #profile = profile.clone()\n",
    "        profile[mask,:]=NLTE.compute_profile(frequencies,line_doppler[mask],sigma[mask]).float()\n",
    "        chi_ij=NLTE.emissivity_and_opacity_ij(line,model,RM.pops)[1].to(dtype=torch.float32)\n",
    "        #chi_ij=chi_ij.clone()\n",
    "        #print('check chi_ij', torch.count_nonzero(chi_ij<0))\n",
    "        #print('check dz',dz[dz<0])\n",
    "        dtau,tau=NLTE.optical_depth_along_last_axis(line,chi_ij,sigma,\n",
    "                                               RM['density'],\n",
    "                                               RM['temperature'],\n",
    "                                               RM['velocity_z:v_max'],\n",
    "                                               frequencies, dz)\n",
    "        tau=tau.to(dtype=torch.float32) \n",
    "        dtau=dtau.to(dtype=torch.float32) \n",
    "        source =NLTE.source_ij(line, RM['density']*RM.pops).to(torch.float32)\n",
    "        #print(f'check source:{torch.isnan(source).any()}')\n",
    "        #print('check dtau',dtau[dtau<0])\n",
    "        img=NLTE.image_along_last_axis(source, dtau, tau)\n",
    "        #print(f'check img:{torch.isnan(img).any()}')\n",
    "        #print('check img',img[img<0])\n",
    "        R_J=torch.trapz(img*profile,frequencies, dim=-1)\n",
    "        J=rotate_scalar(R_J, RM.center, RM.R.T)\n",
    "        #print('check J:', J[J<0])\n",
    "        mean_intensity = mean_intensity+ J \n",
    "        #print('check mean intensity:',mean_intensity[mean_intensity<0])\n",
    "        #mean_intensity = torch.trapz(img*profile,frequencies, dim=-1)\n",
    "    mean_intensity= mean_intensity/ nrays \n",
    "    #mean_intensity=torch.trapz(img*profile,frequencies, dim=-1)\n",
    "    return mean_intensity\n",
    "def transition_Rate(R_ij,C_ij):\n",
    "    P_ij=R_ij+C_ij\n",
    "    return P_ij\n",
    "def compute_SE(model,line,C_ij,C_ji,J,NLTE_pops):\n",
    "    dni_dt=torch.zeros(len(line.linedata.level),*model.shape)\n",
    "    for i in range(1,len(line.linedata.level)):\n",
    "        j=i-1\n",
    "        sum_Pji_nj=0\n",
    "        sum_Pij_ni=0\n",
    "        n_i=(model['density'] * NLTE_pops[i,...])\n",
    "        n_j=(model['density']* NLTE_pops[j,...])\n",
    "        A_ij=line.linedata.Einstein_A[i-1]\n",
    "        B_ji=line.linedata.Einstein_Bs[i-1]\n",
    "        B_ij=line.linedata.Einstein_Ba[i-1]\n",
    "        R_ij=A_ij+B_ij*J\n",
    "        R_ji=B_ji*J  \n",
    "        P_ij=transition_Rate(R_ij,C_ij)\n",
    "        P_ji=transition_Rate(R_ji,C_ji)\n",
    "        sum_Pji_nj=n_j*P_ji\n",
    "        sum_Pij_ni=n_i*P_ij        \n",
    "        dni_dt[i,...]=sum_Pji_nj-sum_Pij_ni   \n",
    "        #print('*********check10****************************')\n",
    "        dni_dt[j,...]=sum_Pij_ni-sum_Pji_nj   \n",
    "    return dni_dt\n",
    "\n",
    "def loss_function(dz,model,line,NLTE_pops):\n",
    "    J=compute_mean_intensity(model,NLTE_pops,line,nfreq,frequencies)\n",
    "    print(f\"J contains NaN: { torch.isnan(J).any()}\")\n",
    "    dni_dt=compute_SE(model,line,C_ij,C_ji,J,NLTE_pops)\n",
    "    print(f\"dni_dt contains NaN: { torch.isnan(dni_dt).any()}\")\n",
    "    #sum_pops = torch.sum(NLTE_pops, dim=0)\n",
    "    #sum_loss = torch.norm(sum_pops- 1, p=2)\n",
    "    L1=torch.norm(dni_dt[0,...]/model['density'],p=1)\n",
    "    L2=torch.norm(dni_dt[1,...]/model['density'],p=1)\n",
    "    \n",
    "    original_loss=L1+L2  \n",
    "    #original_loss=torch.norm(dni_dt[0,...],p=2)\n",
    "    #total_loss = original_loss + sum_loss\n",
    "    return original_loss\n",
    "def transition_Rate(R_ij,C_ij):\n",
    "    P_ij=R_ij+C_ij\n",
    "    return P_ij\n",
    "def compute_SE(model,line,C_ij,C_ji,J,NLTE_pops):\n",
    "    dni_dt=torch.zeros(len(line.linedata.level),*model.shape)\n",
    "    for i in range(1,len(line.linedata.level)):\n",
    "        j=i-1\n",
    "        sum_Pji_nj=0\n",
    "        sum_Pij_ni=0\n",
    "        #n_i=(model['density'] * NLTE_pops[i,...])\n",
    "        #n_j=(model['density']* NLTE_pops[j,...])\n",
    "        n_i=NLTE_pops[i,...]\n",
    "        n_j=NLTE_pops[j,...]\n",
    "        A_ij=line.linedata.Einstein_A[i-1]\n",
    "        B_ij=line.linedata.Einstein_Bs[i-1]\n",
    "        B_ji=line.linedata.Einstein_Ba[i-1]\n",
    "        R_ij=A_ij+B_ij*J\n",
    "        R_ji=B_ji*J  \n",
    "        \n",
    "        P_ij=transition_Rate(R_ij,C_ij)\n",
    "        P_ji=transition_Rate(R_ji,C_ji)\n",
    "        sum_Pji_nj=n_j*P_ji\n",
    "        sum_Pij_ni=n_i*P_ij        \n",
    "        dni_dt[i,...]=sum_Pji_nj-sum_Pij_ni   \n",
    "        #print('*********check10****************************')\n",
    "        dni_dt[j,...]=sum_Pij_ni-sum_Pji_nj   \n",
    "    return dni_dt\n",
    "\n",
    "def compute_SE(model,line,C_ij,C_ji,J,NLTE_pops):\n",
    "    dni_dt=torch.zeros(len(line.linedata.level),*model.shape)\n",
    "    #for i in range(1,len(line.linedata.level)):\n",
    "    i=1\n",
    "    j=i-1\n",
    "    sum_Pji_nj=0\n",
    "    sum_Pij_ni=0\n",
    "    #n_i=(model['density'] * NLTE_pops[i,...])\n",
    "    #n_j=(model['density']* NLTE_pops[j,...])\n",
    "    n_i=NLTE_pops[i,...]\n",
    "    n_j=NLTE_pops[j,...]\n",
    "    A_ij=line.linedata.Einstein_A[i-1]\n",
    "    B_ij=line.linedata.Einstein_Bs[i-1]\n",
    "    B_ji=line.linedata.Einstein_Ba[i-1]\n",
    "    R_ij=A_ij+B_ij*J\n",
    "    R_ji=B_ji*J  \n",
    "    \n",
    "    P_ij=transition_Rate(R_ij,C_ij)\n",
    "    P_ji=transition_Rate(R_ji,C_ji)\n",
    "    sum_Pji_nj=n_j*P_ji\n",
    "    sum_Pij_ni=n_i*P_ij        \n",
    "    dni_dt[i,...]=sum_Pji_nj-sum_Pij_ni   \n",
    "    #print('*********check10****************************')\n",
    "    dni_dt[j,...]=sum_Pij_ni-sum_Pji_nj   \n",
    "    return dni_dt\n",
    "\n",
    "def loss_function(dz,model,line,NLTE_pops):\n",
    "    J=compute_mean_intensity(model,NLTE_pops,line,nfreq,frequencies)\n",
    "    print(f\"J contains NaN: { torch.isnan(J).any()}\")\n",
    "    dni_dt=compute_SE(model,line,C_ij,C_ji,J,NLTE_pops)\n",
    "    print(f\"dni_dt contains NaN: { torch.isnan(dni_dt).any()}\")\n",
    "    #sum_pops = torch.sum(NLTE_pops, dim=0)\n",
    "    #sum_loss = torch.norm(sum_pops- 1, p=2)\n",
    "    L1=torch.norm(dni_dt[0,...],p=2)\n",
    "    L2=torch.norm(dni_dt[1,...],p=2)\n",
    "    original_loss=L1+L2  \n",
    "    #original_loss=torch.norm(dni_dt[0,...],p=2)\n",
    "    #total_loss = original_loss + sum_loss\n",
    "    return original_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d7133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_step(optimizer, params, mask,dz,model,line,r_mask):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    normalized_params = params / torch.sum(params, dim=0, keepdim=True)\n",
    "    params=normalized_params\n",
    "    #loss_function(dz,model,line,NLTE_pops,r_mask)\n",
    "    #loss=loss_function(dz,model,line, params)\n",
    "    \n",
    "    loss = loss_function(dz, model, line, params,r_mask) \n",
    "  \n",
    "    #params=NLTE_pops/ (torch.sum(params, dim=0)[None,...])\n",
    "    loss.backward()\n",
    "  \n",
    "    with torch.no_grad():\n",
    "    #    params.grad[~mask] = 0\n",
    "        optimizer.step()\n",
    "    \n",
    "    #params.copy_(normalized_params)\n",
    "    return loss\n",
    "def loss_function(dz,model,line,NLTE_pops,r_mask):\n",
    "    J=compute_mean_intensity(model,NLTE_pops,line,nfreq,frequencies)\n",
    "    print(f\"J contains NaN: { torch.isnan(J).any()}\")\n",
    "    dni_dt=compute_SE(model,line,C_ij,C_ji,J,NLTE_pops)\n",
    "    print(f\"dni_dt contains NaN: { torch.isnan(dni_dt).any()}\")\n",
    "    #sum_pops = torch.sum(NLTE_pops, dim=0)\n",
    "    #sum_loss = torch.norm(sum_pops[r_mask]- 1, p=1)\n",
    "    #print(f'sum loss : {sum_loss}')\n",
    "    #L1=torch.norm((dni_dt[0,...])[r_mask],p=1)\n",
    "    #L2=torch.norm((dni_dt[1,...])[r_mask],p=1)\n",
    "    L1=torch.norm((dni_dt[0,...][r_mask]),p=1)\n",
    "    L2=torch.norm((dni_dt[1,...][r_mask]),p=1)\n",
    "    original_loss=L1+L2  \n",
    "    #print(f'original_loss: {original_loss}')\n",
    "    #original_loss=torch.norm(dni_dt[0,...],p=2)\n",
    "    #total_loss = original_loss\n",
    "    return original_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c933016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "r_mask = (r >= -r_s) & (r <= r_s)\n",
    "NLTE_mask=r_mask.unsqueeze(0).expand(2, -1, -1, -1)\n",
    "LTE_pops=torch.zeros(len(line.linedata.level),*model.shape)\n",
    "temp_mask=model['temperature']>1e-20\n",
    "LTE_pops[:,temp_mask]=line.LTE_pops(model['temperature'][temp_mask]).float()\n",
    "#NLTE_pops =LTE_pops.clone().detach().requires_grad_(True)\n",
    "NLTE_pops=torch.rand( line.LTE_pops(model['temperature']).shape).detach().requires_grad_(True)\n",
    "\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_para = None\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "epoch = 0\n",
    "NLTE_pops =LTE_pops.clone().detach().requires_grad_(True)\n",
    "NLTE_pops =(0.5*torch.ones(line.LTE_pops(model['temperature']).shape)).detach().requires_grad_(True)\n",
    "\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)  \n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057eb4c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc64ef9c",
   "metadata": {},
   "source": [
    "### Adam p=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc561a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "n_iters=200\n",
    "tol=1e-7\n",
    "\n",
    "diff=torch.zeros(n_iters)\n",
    "n_loss=torch.zeros(n_iters)\n",
    "n_lr=torch.zeros(n_iters)\n",
    "r_mask = (r >= -r_s) & (r <= r_s)\n",
    "NLTE_mask=r_mask.unsqueeze(0).expand(2, -1, -1, -1)\n",
    "LTE_pops=torch.zeros(len(line.linedata.level),*model.shape)\n",
    "temp_mask=model['temperature']>1e-20\n",
    "LTE_pops[:,temp_mask]=line.LTE_pops(model['temperature'][temp_mask]).float()\n",
    "#NLTE_pops =LTE_pops.clone().detach().requires_grad_(True)\n",
    "best_loss = float('inf')\n",
    "best_para = None\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "epoch = 0\n",
    "NLTE_pops =(0.5*torch.ones(line.LTE_pops(model['temperature']).shape)).detach().requires_grad_(True)\n",
    "\n",
    "optimizer = torch.optim.Adam([NLTE_pops], lr=learning_rate)\n",
    "\n",
    "root_dir=f\"./results/Adam/p_1/lr_{learning_rate}\"\n",
    "if not os.path.exists(root_dir):\n",
    "        os.makedirs(root_dir)\n",
    "        print(f\"Folder '{root_dir}' created.\")\n",
    "else:\n",
    "    print(f\"Folder '{root_dir}' already exists.\")\n",
    "\n",
    "tol=1e-5\n",
    "def loss_function(dz,model,line,NLTE_pops,r_mask):\n",
    "    J=compute_mean_intensity(model,NLTE_pops,line,nfreq,frequencies)\n",
    "    print(f\"J contains NaN: { torch.isnan(J).any()}\")\n",
    "    dni_dt=compute_SE(model,line,C_ij,C_ji,J,NLTE_pops)\n",
    "    print(f\"dni_dt contains NaN: { torch.isnan(dni_dt).any()}\")\n",
    "    #sum_pops = torch.sum(NLTE_pops, dim=0)\n",
    "    #sum_loss = torch.norm(sum_pops[r_mask]- 1, p=1)\n",
    "    #print(f'sum loss : {sum_loss}')\n",
    "    #L1=torch.norm((dni_dt[0,...])[r_mask],p=1)\n",
    "    #L2=torch.norm((dni_dt[1,...])[r_mask],p=1)\n",
    "    L1=torch.norm((dni_dt[0,...][r_mask]),p=1)\n",
    "    L2=torch.norm((dni_dt[1,...][r_mask]),p=1)\n",
    "    original_loss=L1+L2  \n",
    "    #print(f'original_loss: {original_loss}')\n",
    "    #original_loss=torch.norm(dni_dt[0,...],p=2)\n",
    "    #total_loss = original_loss\n",
    "    return original_loss\n",
    "for epoch in range(n_iters):\n",
    "    print(f'********************************************epoch:{epoch+1}********************************************')\n",
    "    #loss = custom_step(optimizer, NLTE_pops, mask)\n",
    "    #optimizer, params, mask,dz,model,line\n",
    "    pops_last_step=NLTE_pops.clone().detach()\n",
    "    loss = custom_step(optimizer, NLTE_pops, NLTE_mask,dz,model,line,r_mask)\n",
    "    diff[epoch]=torch.mean(abs(pops_last_step[1,...][r_mask]-NLTE_pops[1,...][r_mask])/NLTE_pops[1,...][r_mask])\n",
    "    n_loss[epoch]=loss.item()\n",
    "    file_path = os.path.join(root_dir, f'vanZadelhoff_1a_Adam_NLTE_lr_{learning_rate}_epoch_{epoch+1}')\n",
    "    np.save(file_path, NLTE_pops.detach().numpy() )\n",
    "    #if loss.item() < tol:\n",
    "    #    print(f\"Early stopping at epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "    #    break\n",
    "    #print(f'epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    print(f'epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    print(f'epoch {epoch+1}, err: {diff[epoch]}')\n",
    "    fig,axs=plt.subplots(3,1,figsize=(8,15))\n",
    "    axs[0].scatter(abs(r[r_mask].numpy().flatten()),NLTE_pops[0,:,:,:][r_mask].detach().numpy().flatten(),label='0')\n",
    "    axs[0].scatter(abs(r[r_mask].numpy().flatten()),NLTE_pops[1,:,:,:][r_mask].detach().numpy().flatten(),label='1')\n",
    "    axs[0].set_xlabel('r[m]')\n",
    "    axs[0].set_ylabel('fractional level population')\n",
    "    axs[0].legend()\n",
    "    axs[0].set_ylim(0, 1)\n",
    "    axs[0].set_title(f'Epoch {epoch+1}')\n",
    "    axs[0].set_yticks(np.arange(0, 1.1, 0.1))\n",
    "    \n",
    "    \n",
    "    axs[1].plot(diff[:epoch].detach().numpy())\n",
    "    axs[1].set_title(f'relative error Epoch {epoch}')\n",
    "    axs[1].set_xlabel('epoch')\n",
    "    axs[1].set_ylabel('relative error')\n",
    "        \n",
    "    axs[2].plot(n_loss[:epoch].detach().numpy())\n",
    "    axs[2].set_title(f'loss Epoch {epoch}')\n",
    "    axs[2].set_xlabel('epoch')\n",
    "    axs[2].set_ylabel('loss value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig_path=os.path.join(root_dir, f'vanZadelhoff_1a_Adam_NLTE_lr_{learning_rate}_epoch_{epoch+1}.png')\n",
    "    plt.savefig(fig_path)\n",
    "    #plt.savefig(f'./SGD/vanZadelhoff_1a_Adam_NLTE_lr_{learning_rate}_epoch_{epoch+1}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f'epoch {epoch+1}', optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    #scheduler.step()\n",
    "    #scheduler.step(loss.item())\n",
    "    n_lr[epoch]=optimizer.param_groups[0]['lr']\n",
    "    \n",
    "re_file_path = os.path.join(root_dir, 'relative_diff')\n",
    "np.save(re_file_path , diff.detach().numpy() )\n",
    "loss_file_path = os.path.join(root_dir, 'loss_value')\n",
    "np.save(loss_file_path , n_loss.detach().numpy() )\n",
    "lr_file_path = os.path.join(root_dir, 'n_lr')\n",
    "np.save(lr_file_path , n_lr.detach().numpy() )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
